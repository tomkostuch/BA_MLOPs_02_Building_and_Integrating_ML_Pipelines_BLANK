{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9d587a2d",
      "metadata": {},
      "source": [
        "<img src=\"../media/LandingPage-Header-RED-CENTRE.jpg\" alt=\"Notebook Banner\" style=\"width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "docker-deployment-introduction",
      "metadata": {},
      "source": [
        "# Deploying Applications with Containers, Testing, and CI/CD\n",
        "\n",
        "## Docker - Local Testing of Your Dockerized Application\n",
        "\n",
        "This section provides a step by step guide on how to containerize your FastAPI application using Docker and test it locally. Docker allows you to package your application and its dependencies into a standardized unit called a container, ensuring that your application runs consistently across different environments.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The primary goal is to demonstrate the process of creating a Docker image for your FastAPI application and running it as a container for local testing. This is a crucial step before deploying your application to production environments. It covers:\n",
        "\n",
        "1.  **Project Preparation:** Ensuring your project directory is correctly structured with all necessary files for Docker to build the image.\n",
        "2.  **Building the Docker Image:** Using the `docker build` command to create a portable image of your application.\n",
        "3.  **Running the Docker Container:** Launching your application inside a Docker container, mapping necessary ports to access it from your host machine.\n",
        "4.  **Local Testing:** Verifying that your FastAPI application is accessible and functional within the Docker container.\n",
        "5.  **Cleanup Best Practices:** Instructions on how to stop and remove containers and images to manage resources effectively.\n",
        "\n",
        "This process is fundamental for achieving reproducibility and consistency in your application deployments.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before proceeding, ensure you have Docker installed and running on your local machine. You can download Docker Desktop from the official Docker website.\n",
        "\n",
        "### Step by Step Guide for Local Docker Testing\n",
        "\n",
        "**Step 1: Prepare Your Project Directory**\n",
        "\n",
        "Ensure that all the files and directories mentioned in your Dockerfile are present in the same directory as your Dockerfile. This is crucial because the `COPY` commands rely on these files existing in the \"build context\" (the directory where your Dockerfile resides).\n",
        "\n",
        "You should have the following files/directories in your project root:\n",
        "\n",
        "-   `Dockerfile` (the definition of your Docker image)\n",
        "-   `requirements.txt` (listing Python dependencies like `fastapi`, `uvicorn`, `mlflow`, `pandas`, etc.)\n",
        "-   `app.py` (your main FastAPI application file, or `app_flexible.py` if you used that name in the previous step)\n",
        "-   A directory named `src/` (containing `config.py` and other modules, if your project structure includes it)\n",
        "-   A directory named `model_store/` (containing your trained model, e.g., `churn_prediction_model_v1.joblib` or any other model artifact your `app.py` loads).\n",
        "\n",
        "Make sure your current working directory in your terminal is the one containing all these files.\n",
        "\n",
        "**Step 2: Build the Docker Image**\n",
        "\n",
        "This command reads your `Dockerfile` and creates a Docker image, which is a lightweight, standalone, executable package of your application.\n",
        "\n",
        "-   Open your terminal or command prompt.\n",
        "-   Navigate to the directory where your `Dockerfile` is located.\n",
        "-   Run the following command:\n",
        "    ```bash\n",
        "    docker build -t churn-fastapi-app .\n",
        "    ```\n",
        "    * `docker build`: This is the command to build a Docker image.\n",
        "    * `-t churn-fastapi-app`: This tags your image with a name (`churn-fastapi-app`). You can choose any meaningful name. This name will be used to reference the image later.\n",
        "    * `.`: This specifies the build context, meaning Docker will look for the `Dockerfile` and all referenced files in the current directory.\n",
        "\n",
        "You will see output in your terminal as Docker executes each step defined in your `Dockerfile`. If successful, it will end with a message indicating the image was built.\n",
        "\n",
        "**Step 3: Run the Docker Container**\n",
        "\n",
        "Once the image is built, you can run a container from it. A container is a running instance of an image.\n",
        "\n",
        "-   In your terminal, run the following command:\n",
        "    ```bash\n",
        "    docker run -p 8000:8000 --name churn-fastapi-app-instance churn-fastapi-app\n",
        "    ```\n",
        "    * `docker run`: This command starts a new container.\n",
        "    * `-p 8000:8000`: This is crucial for local testing. It maps port `8000` on your host machine (your computer) to port `8000` inside the Docker container. Your `Dockerfile` exposes port `8000`, and your `CMD` command runs Uvicorn on port `8000` inside the container. This mapping allows you to access the app from your browser.\n",
        "    * `--name churn-fastapi-app-instance`: This gives your running container a memorable name (`churn-fastapi-app-instance`). This is optional but very useful for stopping or removing it later. You can use `docker ps` to see running containers and their names/IDs.\n",
        "    * `churn-fastapi-app`: This is the name of the Docker image you built in Step 2.\n",
        "\n",
        "The application will start running inside the container, and you should see Uvicorn's output in your terminal, indicating it's listening on `http://0.0.0.0:8000`.\n",
        "\n",
        "**Step 4: Test Your FastAPI Application**\n",
        "\n",
        "Now that your container is running, you can access your FastAPI application from your web browser or using tools like `curl`.\n",
        "\n",
        "-   Open your web browser.\n",
        "-   Go to: `http://localhost:8000/docs`\n",
        "\n",
        "This will show you all your defined API endpoints and allow you to test them directly from the browser using Swagger UI.\n",
        "\n",
        "**Step 5: Stop and Clean Up (Good Practice)**\n",
        "\n",
        "Once you've finished testing, it's good practice to stop and remove the container to free up resources.\n",
        "\n",
        "-   **To stop the container:**\n",
        "    * If you ran it in the foreground (without `-d`), go back to your terminal where the container is running and press `Ctrl+C`.\n",
        "    * Alternatively, if you ran it in detached mode (with `-d`) or in another terminal: `docker stop churn-fastapi-app-instance` (Replace `churn-fastapi-app-instance` with your container's name or ID from `docker ps`).\n",
        "\n",
        "-   **To remove the container (after stopping):**\n",
        "    ```bash\n",
        "    docker rm churn-fastapi-app-instance\n",
        "    ```\n",
        "\n",
        "-   **To remove the Docker image (if you no longer need it):**\n",
        "    ```bash\n",
        "    docker rmi churn-fastapi-app\n",
        "    ```\n",
        "    (This will only work if no containers are currently using the image).\n",
        "\n",
        "By following these steps, you can effectively build and test your Dockerized FastAPI application locally."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dockerfile-example",
      "metadata": {},
      "source": [
        "### Example Dockerfile\n",
        "\n",
        "Below is an example of a `Dockerfile` that you would place in your project's root directory. This `Dockerfile` defines the steps to build your Docker image.\n",
        "\n",
        "```dockerfile\n",
        "# Dockerfile\n",
        "# Use an official Python runtime as a parent image. This provides a base operating system\n",
        "# with Python pre-installed, making the image lightweight and efficient.\n",
        "FROM python:3.11-slim-buster\n",
        "# Using Python 3.11 for compatibility.\n",
        "\n",
        "# Set the working directory inside the container to /app.\n",
        "# All subsequent commands (like COPY, RUN, CMD) will be executed relative to this directory.\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the requirements.txt file from your host machine into the container at /app.\n",
        "# This step is done early to leverage Docker's build cache.\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Install any needed Python packages specified in requirements.txt.\n",
        "# --no-cache-dir: Prevents pip from storing cache, reducing the final image size.\n",
        "# -r: Installs packages listed in the requirements file.\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the main FastAPI application file (e.g., app.py or app_flexible.py) into the container at /app.\n",
        "COPY app.py /app/\n",
        "\n",
        "# Copy the 'src' directory (if it exists) containing config.py and other custom modules.\n",
        "# This is crucial for resolving Python imports like 'from src import config'.\n",
        "COPY src/ /app/src/\n",
        "\n",
        "# Copy the 'model_store' directory containing your trained model artifacts.\n",
        "# Ensure that 'model_store/churn_prediction_model_v1.joblib' (or your actual model file)\n",
        "# exists in your host machine's project directory before building the Docker image.\n",
        "COPY model_store/ /app/model_store/\n",
        "\n",
        "# Expose port 8000. This informs Docker that the container listens on the specified network port\n",
        "# at runtime. It doesn't actually publish the port; it's documentation and can be used by other tools.\n",
        "EXPOSE 8000\n",
        "\n",
        "# Command to run the application using Uvicorn when the container starts.\n",
        "# 'app:app' refers to the 'app' object inside 'app.py' (or 'app_flexible.py').\n",
        "# '--host 0.0.0.0' makes the server accessible from outside the container (from any IP address).\n",
        "# '--port 8000' specifies the port Uvicorn listens on inside the container.\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea0093e2",
      "metadata": {},
      "source": [
        "# Testing Machine Learning Deployments\n",
        "\n",
        "## Comprehensive Testing Strategy for FastAPI ML Applications\n",
        "\n",
        "Testing is a critical phase in the development and deployment of machine learning applications, ensuring that models perform as expected and that the API serving them is robust and reliable. This section outlines a comprehensive testing strategy that includes unit, integration, functional, and end to end (E2E) tests.\n",
        "\n",
        "### Purpose of This Section\n",
        "\n",
        "The primary goal is to demonstrate how to build a testing suite that validates different layers of your ML application:\n",
        "\n",
        "1.  **Unit Tests:** Focusing on isolated components, such as data models (Pydantic schemas) and utility functions, to ensure their internal logic is sound.\n",
        "2.  **Integration Tests:** Verifying the interaction between different components, particularly between the FastAPI API and the loaded machine learning model, ensuring the end to end prediction flow works correctly.\n",
        "3.  **Functional Tests:** Validating the API's behavior from a user's perspective, often by mocking external dependencies like the ML model to ensure correct routing, request/response handling, and error management.\n",
        "4.  **End to End (E2E) Tests:** Testing the entire system as a whole in a production like environment, including the Docker container, FastAPI server, network layer, and the actual machine learning model, to verify full system functionality.\n",
        "\n",
        "This layered approach helps in identifying and isolating bugs early in the development cycle, leading to more stable and maintainable deployments.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Ensure you have `pytest` (or `unittest` as used here) and `fastapi[all]` (which includes `TestClient`) installed:\n",
        "```bash\n",
        "pip install pytest \"fastapi[all]\" # or pip install unittest for standard library\n",
        "```\n",
        "\n",
        "Also, ensure your project structure allows for correct imports. The provided test scripts assume a structure where `app.py` is in the parent directory of `ml_project/tests/`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unit-test-explanation",
      "metadata": {},
      "source": [
        "### Unit Test: `test_api_unit.py`\n",
        "\n",
        "**Objective:** This unit test module is designed to verify the correctness of the `ChurnPredictRequest` Pydantic model (or `TitanicPassenger` in our case, assuming the user's `app.py` has been updated to use `ChurnPredictRequest` as per the test code), which is used in your API to define the structure and data types of incoming prediction requests. The key purpose is to ensure that this data model correctly validates incoming data accepting valid data and rejecting invalid data in complete isolation from the FastAPI server or the machine learning model.\n",
        "\n",
        "**Tests Performed:**\n",
        "\n",
        "* `test_churn_predict_request_validation_success`: Verifies that valid data correctly validates against the Pydantic model.\n",
        "* `test_churn_predict_request_validation_failure_invalid_type`: Tests that the Pydantic model raises an error for incorrect data types.\n",
        "* `test_churn_predict_request_validation_failure_missing_field`: Tests that the Pydantic model raises an error for missing required fields.\n",
        "\n",
        "**How to Run:**\n",
        "\n",
        "1.  Save the code below as `test_api_unit.py` inside a `tests/` directory within your project (e.g., `your_project/tests/test_api_unit.py`).\n",
        "2.  Ensure your `app.py` file (containing the `ChurnPredictRequest` or `TitanicPassenger` Pydantic model) is in the parent directory of `tests/`.\n",
        "3.  From your project's root directory, run:\n",
        "    ```bash\n",
        "    python -m unittest tests/test_api_unit.py\n",
        "    ```\n",
        "    or if using `pytest`:\n",
        "    ```bash\n",
        "    pytest tests/test_api_unit.py\n",
        "    ```\n",
        "\n",
        "This will execute the unit tests and report their success or failure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32796dc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ml_project/tests/test_api_unit.py\n",
        "\"\"\"\n",
        "Objective:\n",
        "    Unit tests specifically for the FastAPI application's Pydantic models\n",
        "    and any small, isolated utility functions within app.py,\n",
        "    implemented using Python's built in `unittest` framework.\n",
        "    It focuses on validating data structures and basic logic in isolation,\n",
        "    without interacting with the FastAPI server or the ML model's prediction logic.\n",
        "\n",
        "Tests Performed:\n",
        "    - test_churn_predict_request_validation_success:\n",
        "        Verifies that valid data correctly validates against the Pydantic model.\n",
        "    - test_churn_predict_request_validation_failure_invalid_type:\n",
        "        Tests that the Pydantic model raises an error for incorrect data types.\n",
        "    - test_churn_predict_request_validation_failure_missing_field:\n",
        "        Tests that the Pydantic model raises an error for missing required fields.\n",
        "\"\"\"\n",
        "import unittest\n",
        "import os\n",
        "import sys\n",
        "from pydantic import ValidationError # Specific exception for Pydantic validation\n",
        "\n",
        "# Add the project root to sys.path to allow imports from app.py\n",
        "# Assumes test_api_unit.py is in 'ml_project/tests/'\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
        "\n",
        "# IMPORTANT: Replace ChurnPredictRequest with TitanicPassenger if that's the name in your app.py\n",
        "from app import ChurnPredictRequest # Import the Pydantic model\n",
        "\n",
        "\n",
        "class TestChurnPredictRequest(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        Set up common valid prediction data for tests.\n",
        "        This data should match the expected schema of your Pydantic model.\n",
        "        \"\"\"\n",
        "        self.valid_prediction_data = {\n",
        "            \"gender\": \"Male\",\n",
        "            \"SeniorCitizen\": 0,\n",
        "            \"Partner\": \"No\",\n",
        "            \"Dependents\": \"No\",\n",
        "            \"tenure\": 1,\n",
        "            \"PhoneService\": \"No\",\n",
        "            \"MultipleLines\": \"No phone service\",\n",
        "            \"InternetService\": \"DSL\",\n",
        "            \"OnlineSecurity\": \"No\",\n",
        "            \"OnlineBackup\": \"Yes\",\n",
        "            \"DeviceProtection\": \"No\",\n",
        "            \"TechSupport\": \"No\",\n",
        "            \"StreamingTV\": \"No\",\n",
        "            \"StreamingMovies\": \"No\",\n",
        "            \"Contract\": \"Month-to-month\",\n",
        "            \"PaperlessBilling\": \"Yes\",\n",
        "            \"PaymentMethod\": \"Electronic check\",\n",
        "            \"MonthlyCharges\": 29.85,\n",
        "            \"TotalCharges\": 29.85\n",
        "        }\n",
        "        print(\"\\n--- Setup: Valid prediction data prepared ---\")\n",
        "\n",
        "    def test_churn_predict_request_validation_success(self):\n",
        "        \"\"\"Test that valid data correctly validates against the Pydantic model.\"\"\"\n",
        "        print(\"\\n--- Test: Valid data validation success ---\")\n",
        "        # The test will fail automatically if a ValidationError is raised here,\n",
        "        # which is the desired behavior for a success test.\n",
        "        request_model = ChurnPredictRequest(**self.valid_prediction_data)\n",
        "\n",
        "        # Assert that the data was loaded correctly into the model's attributes.\n",
        "        self.assertEqual(request_model.gender, \"Male\")\n",
        "        self.assertEqual(request_model.tenure, 1)\n",
        "        self.assertEqual(request_model.TotalCharges, 29.85)\n",
        "        print(\"Pydantic model successfully validated with valid data.\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "    def test_churn_predict_request_validation_failure_invalid_type(self):\n",
        "        \"\"\"Test Pydantic model raises error for incorrect data types.\"\"\"\n",
        "        print(\"\\n--- Test: Validation failure - invalid type ---\")\n",
        "        invalid_data = self.valid_prediction_data.copy()\n",
        "        invalid_data[\"tenure\"] = \"not_an_int\" # Introduce a type mismatch\n",
        "\n",
        "        with self.assertRaises(ValidationError) as cm:\n",
        "            ChurnPredictRequest(**invalid_data)\n",
        "\n",
        "        # Check if the error message contains a relevant type error message\n",
        "        error_message = str(cm.exception)\n",
        "        self.assertIn(\"Input should be a valid integer\", error_message)\n",
        "        print(f\"Pydantic model correctly raised ValidationError for invalid type: {error_message.splitlines()[0]}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "    def test_churn_predict_request_validation_failure_missing_field(self):\n",
        "        \"\"\"Test Pydantic model raises error for missing required fields.\"\"\"\n",
        "        print(\"\\n--- Test: Validation failure - missing field ---\")\n",
        "        invalid_data = self.valid_prediction_data.copy()\n",
        "        del invalid_data[\"gender\"] # Remove a required field\n",
        "\n",
        "        with self.assertRaises(ValidationError) as cm:\n",
        "            ChurnPredictRequest(**invalid_data)\n",
        "\n",
        "        # Check if the error message contains a relevant missing field message\n",
        "        error_message = str(cm.exception)\n",
        "        self.assertIn(\"Field required\", error_message)\n",
        "        self.assertIn(\"gender\", error_message)\n",
        "        print(f\"Pydantic model correctly raised ValidationError for missing field: {error_message.splitlines()[0]}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "integration-test-explanation",
      "metadata": {},
      "source": [
        "### Integration Test: `test_api_integration.py`\n",
        "\n",
        "**Objective:** This integration test module is designed to verify that the API layer (built with FastAPI) and the machine learning model work together correctly. Its main purpose is to simulate real API calls and check if the application behaves as expected, from receiving a request to returning a valid prediction from the actual, loaded model.\n",
        "\n",
        "**Tests Performed:**\n",
        "\n",
        "* `test_predict_endpoint_with_real_model_success`: Verifies that the `/predict` endpoint returns a `200` status code and a valid prediction structure for a typical input, using the actual loaded model.\n",
        "* `test_predict_endpoint_validation_failure`: Verifies that the `/predict` endpoint correctly handles invalid input data by returning a `422` status code (Unprocessable Entity).\n",
        "* `test_predict_endpoint_with_real_model_edge_case_input`: Tests the `/predict` endpoint with an edge case input to ensure robustness.\n",
        "\n",
        "**How to Run:**\n",
        "\n",
        "1.  Save the code below as `test_api_integration.py` inside the same `tests/` directory (e.g., `your_project/tests/test_api_integration.py`).\n",
        "2.  Ensure your `app.py` file is in the parent directory of `tests/` and that the `MODEL_PATH` (or `MLFLOW_RUN_ID` and `MODEL_ARTIFACT_PATH` if using MLflow `load_model`) in `app.py` points to a valid, existing model file.\n",
        "3.  From your project's root directory, run:\n",
        "    ```bash\n",
        "    python -m unittest tests/test_api_integration.py\n",
        "    ```\n",
        "    or if using `pytest`:\n",
        "    ```bash\n",
        "    pytest tests/test_api_integration.py\n",
        "    ```\n",
        "\n",
        "These tests will spin up a test instance of your FastAPI application, send requests to it, and assert on the responses, providing confidence in your API's functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c86fd04",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ml_project/tests/test_api_integration.py\n",
        "\"\"\"\n",
        "Objective:\n",
        "    Integration tests for the FastAPI application, implemented using Python's\n",
        "    built-in `unittest` framework and FastAPI's `TestClient`.\n",
        "    It focuses on validating the interaction between the FastAPI API endpoints\n",
        "    and the actual, loaded machine learning model. These tests ensure that\n",
        "    the model is correctly loaded and can make valid predictions based on\n",
        "    the data received through the API, verifying the end-to-end data flow\n",
        "    from API request parsing to model inference.\n",
        "\n",
        "Tests Performed:\n",
        "    - test_predict_endpoint_with_real_model_success:\n",
        "        Verifies that the /predict endpoint returns a 200 status code and\n",
        "        a valid prediction structure for a typical input, using the actual loaded model.\n",
        "    - test_predict_endpoint_validation_failure:\n",
        "        Verifies that the /predict endpoint correctly handles invalid input data\n",
        "        by returning a 422 status code (Unprocessable Entity).\n",
        "    - test_predict_endpoint_with_real_model_edge_case_input:\n",
        "        Tests the /predict endpoint with an edge case input to ensure robustness.\n",
        "\"\"\"\n",
        "import unittest\n",
        "import os\n",
        "import sys\n",
        "import json # Used if loading data from a file for fixtures, though here it's direct dicts\n",
        "\n",
        "# Import TestClient and app from FastAPI\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "# Add the project root to sys.path to allow imports from app.py\n",
        "# Assumes test_api_integration.py is in 'ml_project/tests/'\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
        "\n",
        "# Import app and MODEL_PATH to check if the real model is loaded\n",
        "# IMPORTANT: If you used MLflow's load_model in app.py, you might need to adjust\n",
        "# how you check for model loading, as app.state might not be directly used.\n",
        "# For MLflow, app.on_event(\"startup\") handles it. This test assumes app.py\n",
        "# stores the loaded model in `app.state.model_pipeline`.\n",
        "from app import app, MODEL_PATH # Assuming MODEL_PATH is defined in app.py for the model file\n",
        "\n",
        "\n",
        "class TestAPIIntegration(unittest.TestCase):\n",
        "\n",
        "    # Class-level setup for the TestClient, equivalent to pytest fixture(scope=\"module\")\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"\n",
        "        Set up the FastAPI TestClient once for all tests in this class.\n",
        "        Also, verifies that the model is loaded during app startup.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Integration Test Setup: Initializing FastAPI TestClient and checking model ---\")\n",
        "        # Initialize TestClient and manually trigger the app's startup events\n",
        "        cls.client = TestClient(app)\n",
        "        cls.client.__enter__() # Manually enter the context manager for startup events\n",
        "\n",
        "        # Verify that the model pipeline is loaded. This check depends on how app.py handles model loading.\n",
        "        # If app.py uses @app.on_event(\"startup\") to load a model into a global variable,\n",
        "        # you might need to adjust this check (e.g., check the global variable directly).\n",
        "        # This assumes the model is stored in app.state.model_pipeline after startup.\n",
        "        if not hasattr(app.state, \"model_pipeline\") or app.state.model_pipeline is None:\n",
        "            # This will fail the test suite early if the model isn't loaded, as expected\n",
        "            raise AssertionError(\n",
        "                f\"Integration test requires actual model loaded at {MODEL_PATH}. \"\n",
        "                \"Please ensure the model file exists and is correctly loaded into app.state by app.py \"\n",
        "                \"during application startup.\")\n",
        "        print(\"FastAPI TestClient initialized. Model confirmed to be loaded.\")\n",
        "\n",
        "        # Define valid and edge-case prediction data as class attributes\n",
        "        # These should match the expected input schema of your API.\n",
        "        cls.valid_prediction_data = {\n",
        "            \"gender\": \"Male\", \"SeniorCitizen\": 0, \"Partner\": \"No\", \"Dependents\": \"No\",\n",
        "            \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\",\n",
        "            \"InternetService\": \"DSL\", \"OnlineSecurity\": \"No\", \"OnlineBackup\": \"Yes\",\n",
        "            \"DeviceProtection\": \"No\", \"TechSupport\": \"No\", \"StreamingTV\": \"No\",\n",
        "            \"StreamingMovies\": \"No\", \"Contract\": \"Month-to-month\",\n",
        "            \"PaperlessBilling\": \"Yes\", \"PaymentMethod\": \"Electronic check\",\n",
        "            \"MonthlyCharges\": 29.85, \"TotalCharges\": 29.85\n",
        "        }\n",
        "\n",
        "        cls.edge_case_prediction_data = {\n",
        "            \"gender\": \"Female\", \"SeniorCitizen\": 1, \"Partner\": \"Yes\", \"Dependents\": \"Yes\",\n",
        "            \"tenure\": 72, \"PhoneService\": \"Yes\", \"MultipleLines\": \"Yes\",\n",
        "            \"InternetService\": \"Fiber optic\", \"OnlineSecurity\": \"Yes\", \"OnlineBackup\": \"Yes\",\n",
        "            \"DeviceProtection\": \"Yes\", \"TechSupport\": \"Yes\", \"StreamingTV\": \"Yes\",\n",
        "            \"StreamingMovies\": \"Yes\", \"Contract\": \"Two year\",\n",
        "            \"PaperlessBilling\": \"No\", \"PaymentMethod\": \"Credit card (automatic)\",\n",
        "            \"MonthlyCharges\": 118.75, \"TotalCharges\": 8684.8\n",
        "        }\n",
        "        print(\"Test data for integration tests prepared.\")\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"\n",
        "        Clean up resources after all tests in the class have run.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Integration Test Teardown: Shutting down FastAPI TestClient ---\")\n",
        "        # Manually trigger the app's shutdown events\n",
        "        cls.client.__exit__(None, None, None) # Exit the context manager\n",
        "        print(\"FastAPI TestClient resources cleaned up.\")\n",
        "\n",
        "    def test_predict_endpoint_with_real_model_success(self):\n",
        "        \"\"\"\n",
        "        Tests the /predict endpoint with a real model using valid input,\n",
        "        ensuring a 200 status code and expected prediction structure.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Test: /predict endpoint with valid input ---\")\n",
        "        response = self.client.post(\"/predict\", json=self.valid_prediction_data)\n",
        "\n",
        "        self.assertEqual(response.status_code, 200, f\"Expected status code 200, got {response.status_code}\")\n",
        "        data = response.json()\n",
        "\n",
        "        self.assertIn(\"prediction\", data, \"Response should contain 'prediction' key\")\n",
        "        # Adjust expected prediction values based on your model's actual output (e.g., 0/1 for Titanic)\n",
        "        # Assuming 'Churn'/'No Churn' for the provided churn example\n",
        "        self.assertIn(data[\"prediction\"], [\"Churn\", \"No Churn\"], \"Prediction should be 'Churn' or 'No Churn'\")\n",
        "        self.assertIn(\"no_churn_probability\", data, \"Response should contain 'no_churn_probability' key\")\n",
        "        self.assertIsInstance(data[\"no_churn_probability\"], float, \"no_churn_probability should be a float\")\n",
        "        self.assertIn(\"churn_probability\", data, \"Response should contain 'churn_probability' key\")\n",
        "        self.assertIsInstance(data[\"churn_probability\"], float, \"churn_probability should be a float\")\n",
        "        self.assertGreaterEqual(data[\"no_churn_probability\"], 0.0)\n",
        "        self.assertLessEqual(data[\"no_churn_probability\"], 1.0)\n",
        "        self.assertGreaterEqual(data[\"churn_probability\"], 0.0)\n",
        "        self.assertLessEqual(data[\"churn_probability\"], 1.0)\n",
        "        print(\"'/predict' endpoint with valid input tested successfully.\")\n",
        "        print(f\"Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "    def test_predict_endpoint_validation_failure(self):\n",
        "        \"\"\"\n",
        "        Tests the /predict endpoint with invalid input data to ensure\n",
        "        it returns a 422 status code (Unprocessable Entity).\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Test: /predict endpoint validation failure ---\")\n",
        "        invalid_data = self.valid_prediction_data.copy()\n",
        "        invalid_data[\"tenure\"] = \"not_an_integer\" # Introduce an invalid data type\n",
        "\n",
        "        response = self.client.post(\"/predict\", json=invalid_data)\n",
        "\n",
        "        self.assertEqual(response.status_code, 422, f\"Expected status code 422 for invalid data, got {response.status_code}\")\n",
        "        data = response.json()\n",
        "        self.assertIn(\"detail\", data, \"Error response should contain 'detail' key\")\n",
        "        self.assertIsInstance(data[\"detail\"], list, \"'detail' should be a list of errors\")\n",
        "        self.assertGreater(len(data[\"detail\"]), 0, \"Error detail list should not be empty\")\n",
        "        self.assertIn(\"Input should be a valid integer\", str(data[\"detail\"]), \"Error message should indicate invalid type\")\n",
        "        print(\"'/predict' endpoint correctly handled invalid input with 422 status.\")\n",
        "        print(f\"Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "    def test_predict_endpoint_with_real_model_edge_case_input(self):\n",
        "        \"\"\"\n",
        "        Tests the /predict endpoint with a real model using an edge case or\n",
        "        potentially challenging input to ensure robustness.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Test: /predict endpoint with edge case input ---\")\n",
        "        response = self.client.post(\"/predict\", json=self.edge_case_prediction_data)\n",
        "\n",
        "        self.assertEqual(response.status_code, 200, f\"Expected status code 200, got {response.status_code}\")\n",
        "        data = response.json()\n",
        "        self.assertIn(\"prediction\", data)\n",
        "        self.assertIn(data[\"prediction\"], [\"Churn\", \"No Churn\"])\n",
        "        self.assertIn(\"no_churn_probability\", data)\n",
        "        self.assertIsInstance(data[\"no_churn_probability\"], float)\n",
        "        print(\"'/predict' endpoint with edge case input tested successfully.\")\n",
        "        print(f\"Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "593abb70",
      "metadata": {},
      "source": [
        "### Functional Test: `test_api_functional.py`\n",
        "\n",
        "**Objective:** The main goal of functional tests is to verify the \"function\" of your API endpoints from a user's perspective. This means checking that for a given request, the API returns the correct HTTP status code and a properly formatted response body.\n",
        "\n",
        "To do this quickly and reliably, these tests isolate the API layer by replacing the actual, complex machine learning model with a \"mock\" or \"stunt double.\" This allows you to test the API's routing, request parsing, response generation, and error handling without depending on the potentially slow or resource intensive machine learning model.\n",
        "\n",
        "**Tests Performed:**\n",
        "\n",
        "* `test_predict_endpoint_success`: Verifies that the `/predict` endpoint returns a `200` status code and a valid prediction structure when the model is loaded and returns a mock prediction.\n",
        "* `test_predict_endpoint_model_not_loaded`: Ensures the `/predict` endpoint handles the scenario where the ML model is not yet loaded, returning a `503` status code.\n",
        "* `test_predict_endpoint_invalid_input_data`: Tests that the `/predict` endpoint returns a `422` status code for missing required fields in the input.\n",
        "* `test_predict_endpoint_incorrect_data_type`: Tests that the `/predict` endpoint returns a `422` status code for incorrect data types in the input fields.\n",
        "* `test_predict_endpoint_model_prediction_error`: Verifies that the `/predict` endpoint gracefully handles exceptions originating from the underlying model's prediction method, returning a `500` status.\n",
        "\n",
        "**How to Run:**\n",
        "\n",
        "1.  Save the code below as `test_api_functional.py` inside your `tests/` directory (e.g., `your_project/tests/test_api_functional.py`).\n",
        "2.  Ensure your `app.py` file is in the parent directory of `tests/`.\n",
        "3.  From your project's root directory, run:\n",
        "    ```bash\n",
        "    python -m unittest tests/test_api_functional.py\n",
        "    ```\n",
        "    or if using `pytest`:\n",
        "    ```bash\n",
        "    pytest tests/test_api_functional.py\n",
        "    ```\n",
        "\n",
        "These tests will help ensure the API's behavior is correct regardless of the underlying model's specific output, focusing purely on the API's contract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae90bdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ml_project/tests/test_api_functional.py\n",
        "\"\"\"\n",
        "Objective:\n",
        "    Functional tests for the FastAPI application's API endpoints,\n",
        "    implemented using Python's built-in `unittest` framework and FastAPI's `TestClient`.\n",
        "    It focuses on verifying that each endpoint (`/predict`) behaves as expected\n",
        "    given specific inputs, returning correct HTTP status codes and response bodies.\n",
        "    The ML model's prediction logic is typically mocked in these tests to isolate\n",
        "    the API's routing and response handling from the model's internal logic.\n",
        "\n",
        "Tests Performed:\n",
        "    - test_predict_endpoint_success:\n",
        "        Verifies that the /predict endpoint returns a 200 status code and\n",
        "        a valid prediction structure when the model is loaded and returns a mock prediction.\n",
        "    - test_predict_endpoint_model_not_loaded:\n",
        "        Ensures the /predict endpoint handles the scenario where the ML model\n",
        "        is not yet loaded, returning a 503 status code.\n",
        "    - test_predict_endpoint_invalid_input_data:\n",
        "        Tests that the /predict endpoint returns a 422 status code for missing\n",
        "        required fields in the input.\n",
        "    - test_predict_endpoint_incorrect_data_type:\n",
        "        Tests that the /predict endpoint returns a 422 status code for incorrect\n",
        "        data types in the input fields.\n",
        "    - test_predict_endpoint_model_prediction_error:\n",
        "        Verifies that the /predict endpoint gracefully handles exceptions\n",
        "        originating from the underlying model's prediction method, returning a 500 status.\n",
        "\"\"\"\n",
        "import unittest\n",
        "import os\n",
        "import sys\n",
        "from unittest.mock import MagicMock, patch\n",
        "\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "# Add the project root to sys.path to allow imports from app.py\n",
        "# Assumes test_api_functional.py is in 'ml_project/tests/'\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
        "\n",
        "# Import app as app_module to patch its internal attributes more easily if needed,\n",
        "# and also import the app instance directly.\n",
        "import app as app_module\n",
        "from app import app # Import the FastAPI app instance\n",
        "\n",
        "\n",
        "class TestAPIFunctional(unittest.TestCase):\n",
        "\n",
        "    # Class-level setup for the TestClient\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"\n",
        "        Set up the FastAPI TestClient once for all tests in this class.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Functional Test Setup: Initializing FastAPI TestClient ---\")\n",
        "        # Initialize TestClient and manually trigger the app's startup events\n",
        "        cls.client = TestClient(app)\n",
        "        cls.client.__enter__()\n",
        "        # Define common valid prediction data\n",
        "        cls.valid_prediction_data = {\n",
        "            \"gender\": \"Male\", \"SeniorCitizen\": 0, \"Partner\": \"No\", \"Dependents\": \"No\",\n",
        "            \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\",\n",
        "            \"InternetService\": \"DSL\", \"OnlineSecurity\": \"No\", \"OnlineBackup\": \"Yes\",\n",
        "            \"DeviceProtection\": \"No\", \"TechSupport\": \"No\", \"StreamingTV\": \"No\",\n",
        "            \"StreamingMovies\": \"No\", \"Contract\": \"Month-to-month\",\n",
        "            \"PaperlessBilling\": \"Yes\", \"PaymentMethod\": \"Electronic check\",\n",
        "            \"MonthlyCharges\": 29.85, \"TotalCharges\": 29.85\n",
        "        }\n",
        "        print(\"FastAPI TestClient initialized. Valid prediction data prepared.\")\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"\n",
        "        Clean up resources after all tests in the class have run.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Functional Test Teardown: Shutting down FastAPI TestClient ---\")\n",
        "        # Manually trigger the app's shutdown events\n",
        "        cls.client.__exit__(None, None, None)\n",
        "        print(\"FastAPI TestClient resources cleaned up.\")\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        Set up a fresh mock for the model pipeline before each test.\n",
        "        This ensures tests are isolated from each other's changes to the mock.\n",
        "        \"\"\"\n",
        "        print(\"--- Per-test Setup: Setting up mock model pipeline ---\")\n",
        "        # Create a mock pipeline object for each test\n",
        "        self.mock_pipeline = MagicMock()\n",
        "        # Configure mock with predictable output for successful prediction tests\n",
        "        self.mock_pipeline.predict.return_value = [1] # Predicts 'Churn'\n",
        "        self.mock_pipeline.predict_proba.return_value = [[0.210779946276635, 0.789220053723365]] # [P(No Churn), P(Churn)]\n",
        "\n",
        "        # Use patch.object to replace the model in the app's state.\n",
        "        # self.addCleanup ensures that the patch is automatically stopped after the test.\n",
        "        patcher = patch.object(app_module.app.state, 'model_pipeline', self.mock_pipeline)\n",
        "        patcher.start()\n",
        "        self.addCleanup(patcher.stop)\n",
        "        print(\"Mock model pipeline set as app.state.model_pipeline.\")\n",
        "\n",
        "\n",
        "    def test_predict_endpoint_success(self):\n",
        "        \"\"\"\n",
        "        Test /predict endpoint with valid data when the model is mocked\n",
        "        to return a successful prediction.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Test: /predict endpoint success with mocked model ---\")\n",
        "        response = self.client.post(\"/predict\", json=self.valid_prediction_data)\n",
        "\n",
        "        self.assertEqual(response.status_code, 200, f\"Expected status 200, got {response.status_code}\")\n",
        "        data = response.json()\n",
        "\n",
        "        self.assertIn(\"prediction\", data)\n",
        "        self.assertEqual(data[\"prediction\"], \"Churn\") # Based on mock_pipeline.predict.return_value\n",
        "        self.assertIn(\"no_churn_probability\", data)\n",
        "        self.assertIsInstance(data[\"no_churn_probability\"], float)\n",
        "        self.assertIn(\"churn_probability\", data)\n",
        "        self.assertIsInstance(data[\"churn_probability\"], float)\n",
        "        self.assertEqual(data[\"no_churn_probability\"], 0.210779946276635)\n",
        "        self.assertEqual(data[\"churn_probability\"], 0.789220053723365)\n",
        "        self.mock_pipeline.predict.assert_called_once()\n",
        "        self.mock_pipeline.predict_proba.assert_called_once()\n",
        "        print(\"'/predict' endpoint tested successfully with mocked model.\")\n",
        "        print(f\"Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "    def test_predict_endpoint_model_not_loaded(self):\n",
        "        \"\"\"\n",
        "        Test /predict endpoint when the model pipeline is explicitly set to None.\n",
        "        Should return 503 Service Unavailable.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Test: /predict endpoint - model not loaded ---\")\n",
        "        app_module.app.state.model_pipeline = None # Simulate model not being loaded\n",
        "\n",
        "        response = self.client.post(\"/predict\", json=self.valid_prediction_data)\n",
        "\n",
        "        self.assertEqual(response.status_code, 503, f\"Expected status 503, got {response.status_code}\")\n",
        "        data = response.json()\n",
        "        self.assertIn(\"detail\", data)\n",
        "        self.assertEqual(data[\"detail\"], \"Model not loaded yet.\")\n",
        "        print(\"'/predict' endpoint correctly returned 503 when model not loaded.\")\n",
        "        print(f\"Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "    def test_predict_endpoint_invalid_input_data(self):\n",
        "        \"\"\"Test /predict endpoint with missing required field.\"\"\"\n",
        "        print(\"\\n--- Test: /predict endpoint - invalid input (missing field) ---\")\n",
        "        invalid_data = self.valid_prediction_data.copy()\n",
        "        del invalid_data[\"gender\"] # Missing required field\n",
        "\n",
        "        response = self.client.post(\"/predict\", json=invalid_data)\n",
        "\n",
        "        self.assertEqual(response.status_code, 422, f\"Expected status 422, got {response.status_code}\")\n",
        "        data = response.json()\n",
        "        self.assertIn(\"detail\", data)\n",
        "        self.assertIsInstance(data[\"detail\"], list)\n",
        "        self.assertIn(\"Field required\", str(data[\"detail\"]))\n",
        "        self.assertIn(\"gender\", str(data[\"detail\"]))\n",
        "        print(\"'/predict' endpoint correctly returned 422 for missing field.\")\n",
        "        print(f\"Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "    def test_predict_endpoint_incorrect_data_type(self):\n",
        "        \"\"\"Test /predict endpoint with wrong data type for field.\"\"\"\n",
        "        print(\"\\n--- Test: /predict endpoint - invalid input (incorrect type) ---\")\n",
        "        invalid_data = self.valid_prediction_data.copy()\n",
        "        invalid_data[\"tenure\"] = \"one\" # Incorrect type\n",
        "\n",
        "        response = self.client.post(\"/predict\", json=invalid_data)\n",
        "\n",
        "        self.assertEqual(response.status_code, 422, f\"Expected status 422, got {response.status_code}\")\n",
        "        data = response.json()\n",
        "        self.assertIn(\"detail\", data)\n",
        "        self.assertIsInstance(data[\"detail\"], list)\n",
        "        self.assertIn(\"Input should be a valid integer\", str(data[\"detail\"]))\n",
        "        print(\"'/predict' endpoint correctly returned 422 for incorrect data type.\")\n",
        "        print(f\"Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "    def test_predict_endpoint_model_prediction_error(self):\n",
        "        \"\"\"Test /predict if the model.predict method raises an error.\"\"\"\n",
        "        print(\"\\n--- Test: /predict endpoint - model prediction error ---\")\n",
        "        # Configure the mock model to raise an exception during prediction\n",
        "        self.mock_pipeline.predict.side_effect = Exception(\"Mock prediction error\")\n",
        "        # Ensure the app is using this mock\n",
        "        app_module.app.state.model_pipeline = self.mock_pipeline\n",
        "\n",
        "        response = self.client.post(\"/predict\", json=self.valid_prediction_data)\n",
        "\n",
        "        self.assertEqual(response.status_code, 500, f\"Expected status 500, got {response.status_code}\")\n",
        "        data = response.json()\n",
        "        self.assertIn(\"detail\", data)\n",
        "        # Check for a more specific error message that indicates a prediction failure\n",
        "        self.assertIn(\"Prediction failed\", data[\"detail\"])\n",
        "        self.assertIn(\"Mock prediction error\", data[\"detail\"])\n",
        "        self.mock_pipeline.predict.assert_called_once()\n",
        "        print(\"'/predict' endpoint correctly returned 500 for model prediction error.\")\n",
        "        print(f\"Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4e2bb0",
      "metadata": {},
      "source": [
        "### End to End (E2E) Test: `test_e2e_suite.py`\n",
        "\n",
        "**Objective:** The purpose of End to End (E2E) tests is to verify that your entire API application works correctly from start to finish in a production like environment. It tests the complete system as a whole, including the Docker container, the FastAPI server, the network layer, and the actual machine learning model.\n",
        "\n",
        "These tests are the most comprehensive, as they validate the entire deployment pipeline, from containerization to live inference. They ensure that all components interact seamlessly and that the deployed application meets its functional requirements in a realistic setting.\n",
        "\n",
        "**Tests Performed:**\n",
        "\n",
        "* `test_e2e_predict_success`: Deploys the FastAPI app in Docker, makes a valid prediction request to its API, and asserts on the `200` status code and the structure/values of the prediction response.\n",
        "* `test_e2e_predict_invalid_input`: Deploys the FastAPI app, makes a prediction request with invalid input, and asserts on the `422` status code and the error details.\n",
        "\n",
        "**How to Run:**\n",
        "\n",
        "1.  Save the code below as `test_e2e_suite.py` inside your `tests/` directory (e.g., `your_project/tests/test_e2e_suite.py`).\n",
        "2.  Ensure you have a `Dockerfile`, `app.py`, `requirements.txt`, and your `model_store/` directory (with the model file) in your project's root directory, as these are needed for Docker to build the image.\n",
        "3.  Ensure Docker Desktop or Docker Engine is running on your machine.\n",
        "4.  From your project's root directory, run:\n",
        "    ```bash\n",
        "    python -m unittest tests/test_e2e_suite.py\n",
        "    ```\n",
        "    or if using `pytest`:\n",
        "    ```bash\n",
        "    pytest tests/test_e2e_suite.py\n",
        "    ```\n",
        "\n",
        "These tests will automatically build a Docker image, run a container, send HTTP requests, and then clean up the Docker resources, providing a full end to end validation of your deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a9ca39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# churn_classification_app_unittest/tests_unittest/test_e2e_suite.py\n",
        "\"\"\"\n",
        "Objective:\n",
        "    End-to-End (E2E) tests for the deployed FastAPI application,\n",
        "    implemented using Python's built-in `unittest` framework.\n",
        "    It orchestrates the setup of a test environment, including:\n",
        "    1. Verifying the existence of a pre-trained ML model on the host.\n",
        "    2. Building a Docker image for the FastAPI app.\n",
        "    3. Starting the Docker container based on the built image.\n",
        "    4. Waiting for the API service to become reachable.\n",
        "    5. Executing actual HTTP requests against the running service.\n",
        "    6. Performing necessary cleanup (stopping/removing container, deleting Docker image).\n",
        "\n",
        "    These tests verify the complete system behavior, including interaction with\n",
        "    the actual loaded ML model and the network layer, mimicking a real-world scenario.\n",
        "\n",
        "Tests Performed:\n",
        "    - test_e2e_predict_success:\n",
        "        Deploys the FastAPI app in Docker, makes a valid prediction request to its API,\n",
        "        and asserts on the 200 status code and the structure/values of the prediction response.\n",
        "    - test_e2e_predict_invalid_input:\n",
        "        Deploys the FastAPI app, makes a prediction request with invalid input,\n",
        "        and asserts on the 422 status code and the error details.\n",
        "\"\"\"\n",
        "import unittest\n",
        "import requests\n",
        "import time\n",
        "import subprocess\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import sys\n",
        "# shutil is no longer needed as we are not deleting model_store or log files\n",
        "# from the host, but managing Docker artifacts.\n",
        "\n",
        "# Add project root to sys.path\n",
        "# Use pathlib for robust path handling\n",
        "PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent\n",
        "sys.path.insert(0, str(PROJECT_ROOT)) # sys.path needs a string\n",
        "\n",
        "# Import config for consistent paths (assuming src.config exists)\n",
        "try:\n",
        "    from src.config import MODEL_FILENAME, MODEL_STORE_DIR # LOG_FILENAME removed\n",
        "except ImportError:\n",
        "    # Fallback for testing if config is not strictly available or if we're in a strange env\n",
        "    print(\"Warning: Could not import MODEL_FILENAME or MODEL_STORE_DIR from src.config. Using defaults.\")\n",
        "    MODEL_FILENAME = \"churn_prediction_model_v1.joblib\"\n",
        "    MODEL_STORE_DIR = \"model_store\"\n",
        "\n",
        "\n",
        "# Configuration for the E2E test\n",
        "API_URL = \"http://localhost:8000\"\n",
        "PREDICT_ENDPOINT = f\"{API_URL}/predict\"\n",
        "DOCKER_IMAGE_NAME = \"churn-api-e2e-test\"\n",
        "CONTAINER_NAME = \"churn-api-e2e-container\"\n",
        "API_STARTUP_TIMEOUT = 120 # seconds for API to become reachable\n",
        "API_HEALTH_CHECK_INTERVAL = 2 # seconds for checking health\n",
        "\n",
        "# Paths for model relative to the project root on the HOST machine\n",
        "MODEL_DIR = PROJECT_ROOT / MODEL_STORE_DIR\n",
        "MODEL_PATH_FULL = MODEL_DIR / MODEL_FILENAME\n",
        "\n",
        "# Sample data for a valid prediction request\n",
        "sample_e2e_prediction_data = {\n",
        "    \"tenure\": 1,\n",
        "    \"MonthlyCharges\": 29.85,\n",
        "    \"TotalCharges\": 29.85,\n",
        "    \"gender\": \"Female\",\n",
        "    \"SeniorCitizen\": 0,\n",
        "    \"Partner\": \"Yes\",\n",
        "    \"Dependents\": \"No\",\n",
        "    \"PhoneService\": \"No\",\n",
        "    \"MultipleLines\": \"No phone service\",\n",
        "    \"InternetService\": \"DSL\",\n",
        "    \"OnlineSecurity\": \"No\",\n",
        "    \"OnlineBackup\": \"Yes\",\n",
        "    \"DeviceProtection\": \"No\",\n",
        "    \"TechSupport\": \"No\",\n",
        "    \"StreamingTV\": \"No\",\n",
        "    \"StreamingMovies\": \"No\",\n",
        "    \"Contract\": \"Month-to-month\",\n",
        "    \"PaperlessBilling\": \"Yes\",\n",
        "    \"PaymentMethod\": \"Electronic check\"\n",
        "}\n",
        "\n",
        "\n",
        "class TestE2ESuite(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"\n",
        "        Sets up the E2E test environment by:\n",
        "        1. Verifying the pre-trained ML model exists on the host (prerequisite for Docker build).\n",
        "        2. Builds the Docker image for the FastAPI app.\n",
        "        3. Starts the Docker container.\n",
        "        4. Waits for the API to be reachable (via /docs endpoint).\n",
        "        \"\"\"\n",
        "        print(\"\\n--- E2E Setup: Preparing environment and starting services ---\")\n",
        "\n",
        "        # 1. Verifying the pre-trained ML model exists on the host.\n",
        "        # This is a prerequisite for the Docker build process to copy it.\n",
        "        print(\"\\n--- E2E Setup: Verifying Pre-trained Model on Host ---\")\n",
        "        if not MODEL_PATH_FULL.exists():\n",
        "            raise AssertionError(\n",
        "                f\"Pre-trained model not found at {MODEL_PATH_FULL}. \"\n",
        "                \"Please ensure the ML pipeline has been run and the model is saved to this location \"\n",
        "                \"before running E2E tests, as it needs to be copied into the Docker image.\"\n",
        "            )\n",
        "        print(f\"Using pre-trained model found on host at: {MODEL_PATH_FULL}\")\n",
        "\n",
        "        # Ensure no pre-existing container with the same name is running\n",
        "        print(\"\\n--- E2E Setup: Cleaning up any old container/image ---\")\n",
        "        subprocess.run([\"docker\", \"stop\", CONTAINER_NAME], check=False, capture_output=True, text=True)\n",
        "        subprocess.run([\"docker\", \"rm\", CONTAINER_NAME], check=False, capture_output=True, text=True)\n",
        "        subprocess.run([\"docker\", \"rmi\", DOCKER_IMAGE_NAME], check=False, capture_output=True, text=True)\n",
        "        print(\"Ensured no pre-existing container or image with the same name.\")\n",
        "\n",
        "\n",
        "        # 2. Builds the Docker image for the FastAPI app.\n",
        "        print(\"\\n--- E2E Setup: Building Docker Image ---\")\n",
        "        try:\n",
        "            # Build the Docker image from the project root (where Dockerfile, app.py, src/, model_store/ are)\n",
        "            # check=True raises CalledProcessError if command returns non-zero exit status\n",
        "            result = subprocess.run([\"docker\", \"build\", \"-t\", DOCKER_IMAGE_NAME, \".\"],\n",
        "                                     check=True, cwd=PROJECT_ROOT, capture_output=True, text=True)\n",
        "            print(\"Docker build stdout:\\n\", result.stdout)\n",
        "            if result.stderr: # Only print stderr if there's actual content\n",
        "                print(\"Docker build stderr:\\n\", result.stderr)\n",
        "            print(f\"Docker image '{DOCKER_IMAGE_NAME}' built successfully.\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            raise AssertionError(f\"Docker image build failed. STDOUT: {e.stdout}\\nSTDERR: {e.stderr}\")\n",
        "        except FileNotFoundError:\n",
        "            raise AssertionError(\"Docker command not found. Is Docker installed and in your system's PATH?\")\n",
        "\n",
        "\n",
        "        # 3. Starts the Docker container.\n",
        "        print(\"\\n--- E2E Setup: Starting Docker Container ---\")\n",
        "        try:\n",
        "            cmd = [\n",
        "                \"docker\", \"run\", \"-d\", # Run in detached mode\n",
        "                \"--name\", CONTAINER_NAME,\n",
        "                \"-p\", \"8000:8000\", # Map host port 8000 to container port 8000\n",
        "                DOCKER_IMAGE_NAME # Use the built image\n",
        "            ]\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
        "            print(f\"Container start stdout: {result.stdout}\")\n",
        "            if result.stderr: # Only print stderr if there's actual content\n",
        "                print(f\"Container start stderr: {result.stderr}\")\n",
        "            cls.container_id = result.stdout.strip()\n",
        "            print(f\"Container {cls.container_id} started.\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Docker run failed. STDOUT: {e.stdout}\\nSTDERR: {e.stderr}\")\n",
        "            cls.tearDownClass() # Attempt cleanup before raising\n",
        "            raise AssertionError(f\"Docker container failed to start: {e}\")\n",
        "        except Exception as e: # Catch any other run-related errors\n",
        "            cls.tearDownClass() # Attempt cleanup before raising\n",
        "            raise AssertionError(f\"An unexpected error occurred during Docker container startup: {e}\")\n",
        "\n",
        "        # 4. Waits for the API to be reachable (e.g., by checking the /docs endpoint).\n",
        "        print(\"Waiting for API to become reachable...\")\n",
        "        for i in range(API_STARTUP_TIMEOUT // API_HEALTH_CHECK_INTERVAL):\n",
        "            try:\n",
        "                # Use a higher timeout for the request itself, separate from total startup timeout\n",
        "                response = requests.get(f\"{API_URL}/docs\", timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    print(\"API is reachable!\")\n",
        "                    break\n",
        "            except requests.exceptions.ConnectionError as e:\n",
        "                print(f\"Connection error: {e}. Retrying...\")\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(\"Request timed out. Retrying...\")\n",
        "            except Exception as e: # Catch any other request-related errors\n",
        "                print(f\"An unexpected error occurred during health check: {e}. Retrying...\")\n",
        "\n",
        "            time.sleep(API_HEALTH_CHECK_INTERVAL)\n",
        "        else: # This block runs if the loop completes without a 'break'\n",
        "            # If the API did not become ready, get logs before failing.\n",
        "            print(\"\\n--- Docker Container Logs (from failed startup) ---\")\n",
        "            logs_result = subprocess.run(\n",
        "                [\"docker\", \"logs\", CONTAINER_NAME],\n",
        "                capture_output=True, text=True, check=False\n",
        "            )\n",
        "            print(logs_result.stderr + logs_result.stdout) # Print both stdout and stderr logs\n",
        "            print(\"---------------------------------------------------\\n\")\n",
        "            cls.tearDownClass() # Ensure cleanup before failing\n",
        "            raise AssertionError(f\"API did not become reachable within {API_STARTUP_TIMEOUT} seconds.\")\n",
        "\n",
        "        print(\"--- E2E Setup Complete ---\")\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"\n",
        "        Teardown for all E2E tests:\n",
        "        1. Stop and remove the Docker container.\n",
        "        2. Remove the Docker image.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- E2E Teardown: Stopping services and cleaning up artifacts ---\")\n",
        "\n",
        "        # 1. Stop and remove Docker container\n",
        "        if hasattr(cls, 'container_id') and cls.container_id:\n",
        "            print(f\"Stopping and removing Docker container: {CONTAINER_NAME} ({cls.container_id[:12]}) ...\")\n",
        "            subprocess.run([\"docker\", \"stop\", CONTAINER_NAME], check=False, capture_output=True, text=True)\n",
        "            subprocess.run([\"docker\", \"rm\", CONTAINER_NAME], check=False, capture_output=True, text=True)\n",
        "            print(\"Docker container stopped and removed.\")\n",
        "\n",
        "        # 2. Remove Docker image\n",
        "        print(f\"Removing Docker image: {DOCKER_IMAGE_NAME}...\")\n",
        "        try:\n",
        "            # -f (force) is often useful for test cleanup to remove even if in use (though it shouldn't be)\n",
        "            subprocess.run([\"docker\", \"rmi\", DOCKER_IMAGE_NAME, \"-f\"], check=False, capture_output=True, text=True)\n",
        "            print(\"Docker image removed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during image removal: {e}\") # Don't fail the test cleanup for this\n",
        "\n",
        "        print(\"--- E2E Teardown Complete ---\")\n",
        "\n",
        "\n",
        "    def test_e2e_predict_success(self):\n",
        "        \"\"\"E2E test for the /predict endpoint with valid data, matching the provided output format.\"\"\"\n",
        "        print(\"\\n--- Test: E2E /predict successful ---\")\n",
        "        response = requests.post(PREDICT_ENDPOINT, json=sample_e2e_prediction_data)\n",
        "\n",
        "        self.assertEqual(response.status_code, 200, f\"Expected status 200, got {response.status_code}. Response: {response.text}\")\n",
        "        data = response.json()\n",
        "\n",
        "        # Assert prediction string\n",
        "        self.assertIn(\"prediction\", data)\n",
        "        self.assertIn(data[\"prediction\"], [\"Churn\", \"No Churn\"], \"Prediction should be 'Churn' or 'No Churn'\")\n",
        "\n",
        "        # Assert direct probability keys\n",
        "        self.assertIn(\"churn_probability\", data)\n",
        "        self.assertIsInstance(data[\"churn_probability\"], float)\n",
        "        self.assertGreaterEqual(data[\"churn_probability\"], 0.0)\n",
        "        self.assertLessEqual(data[\"churn_probability\"], 1.0)\n",
        "\n",
        "        self.assertIn(\"no_churn_probability\", data)\n",
        "        self.assertIsInstance(data[\"no_churn_probability\"], float)\n",
        "        self.assertGreaterEqual(data[\"no_churn_probability\"], 0.0)\n",
        "        self.assertLessEqual(data[\"no_churn_probability\"], 1.0)\n",
        "\n",
        "        # Assert the 'probabilities' dictionary\n",
        "        self.assertIn(\"probabilities\", data)\n",
        "        self.assertIsInstance(data[\"probabilities\"], dict)\n",
        "        self.assertIn(\"No Churn\", data[\"probabilities\"])\n",
        "        self.assertIn(\"Churn\", data[\"probabilities\"])\n",
        "        self.assertIsInstance(data[\"probabilities\"][\"No Churn\"], float)\n",
        "        self.assertIsInstance(data[\"probabilities\"][\"Churn\"], float)\n",
        "        self.assertGreaterEqual(data[\"probabilities\"][\"No Churn\"], 0.0)\n",
        "        self.assertLessEqual(data[\"probabilities\"][\"No Churn\"], 1.0)\n",
        "        self.assertGreaterEqual(data[\"probabilities\"][\"Churn\"], 0.0)\n",
        "        self.assertLessEqual(data[\"probabilities\"][\"Churn\"], 1.0)\n",
        "\n",
        "        # Assert probabilities sum to ~1.0\n",
        "        self.assertAlmostEqual(data[\"churn_probability\"] + data[\"no_churn_probability\"], 1.0, places=5)\n",
        "        self.assertAlmostEqual(data[\"probabilities\"][\"No Churn\"] + data[\"probabilities\"][\"Churn\"], 1.0, places=5)\n",
        "\n",
        "\n",
        "        print(f\"E2E /predict successful. Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "\n",
        "    def test_e2e_predict_invalid_input(self):\n",
        "        \"\"\"E2E test for /predict with invalid input.\"\"\"\n",
        "        print(\"\\n--- Test: E2E /predict invalid input ---\")\n",
        "        invalid_data = sample_e2e_prediction_data.copy()\n",
        "        invalid_data[\"tenure\"] = \"not_a_number\" # Invalid type\n",
        "\n",
        "        response = requests.post(PREDICT_ENDPOINT, json=invalid_data)\n",
        "        self.assertEqual(response.status_code, 422, f\"Expected status 422, got {response.status_code}. Response: {response.text}\")\n",
        "        data = response.json()\n",
        "        self.assertIn(\"detail\", data)\n",
        "        # Check for a validation error related to the 'tenure' field being an invalid integer.\n",
        "        self.assertTrue(\n",
        "            any(\"valid integer\" in err[\"msg\"].lower() and err[\"loc\"][-1] == \"tenure\"\n",
        "                for err in data[\"detail\"]),\n",
        "            f\"Expected validation error for 'tenure' field, got: {data['detail']}\"\n",
        "        )\n",
        "        print(f\"E2E /predict correctly handled invalid input. Response: {data}\")\n",
        "        print(\"--- Test Complete ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58e9dadf",
      "metadata": {},
      "source": [
        "-----------------------------"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "id": "c9aeceb7",
      "metadata": {},
      "source": [
        "# Deploying CI/CD pipeline in GitHub\n",
        "\n",
        "To deploy CI/DC on GitHub the following configurations are needed:\n",
        "\n",
        "- Establish a GitHub repository\n",
        "- Create Your Workflow File: Create a new directory at the root of your repository called `.github/workflows`. Inside this directory, create a new `YAML` file. The name of this file will be the name of your workflow (e.g., `ml_pipeline.yml`, `model_deploy.yml`). This file is where you define the entire CI/CD process. E.g.,\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "- Define Workflow Triggers (`on`): Within your YAML file, the first thing to configure is when the workflow should run. Example:\n",
        "    - on: `push`: The workflow runs every time code is pushed to the repository (you can specify branches, e.g., branches: [main]).\n",
        "    - on: `pull_request`: The workflow runs when a pull request is opened, synchronised, or re-opened.\n",
        "    - on: `schedule`: You can set it to run at specific intervals, for instance, daily at a certain time to retrain a model.\n",
        "    - on: `workflow_dispatch`: Allows for manual triggering from the \"Actions\" tab.\n",
        "\n",
        "- Define Jobs (`jobs`): Workflows are made up of one or more \"jobs.\" Each job runs on a separate virtual machine (called a \"runner\"). You'll define distinct jobs for different phases of your pipeline, e.g., `data_preprocessing`, `model_training`, `model_testing`, `model_deployment`. You specify the operating system for the runner, e.g., `runs-on: ubuntu-latest`.\n",
        "\n",
        "- Define Steps within Jobs (`steps`): Each job consists of a sequence of \"steps.\" These steps are individual tasks that are executed in order. Common Steps for ML CI/CD:\n",
        "\n",
        "    - Checkout Code: Use the `actions/checkout@v4` Action to get your repository's code onto the runner.\n",
        "\n",
        "    - Set up Environment: Use `actions/setup-python@v5` to configure the Python version required.\n",
        "\n",
        "    - Install Dependencies: Run commands like `pip install -r requirements.txt` to install necessary libraries.\n",
        "\n",
        "    - Run Scripts: Execute your Python scripts for data loading, feature engineering, model training, and evaluation (e.g., `python src/train_model.py`).\n",
        "\n",
        "    - Run Tests: Execute unit tests, integration tests, and model performance tests (e.g., `pytest`).\n",
        "\n",
        "    - Build/Push Docker Images: If you're containerising your model.\n",
        "\n",
        "\n",
        "- Monitor Your Pipeline: Once configured, commit your YAML file to the repository. The workflow will trigger based on your defined `on` events. Go to the \"Actions\" tab in your GitHub repository. Here, you will see a list of all your workflow runs, their status (success, failure, in progress), and detailed logs for each step. This is your primary dashboard for observing the CI/CD process.\n",
        "\n",
        "Here below an example of the YAML file for our project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aaba58d",
      "metadata": {},
      "source": [
        "```\n",
        "name: Churn Classification APP CI/CD (Unittest)\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches: [main]\n",
        "    paths:\n",
        "      - '02 Building & Integrating ML Pipelines/churn_classification_app_unittest/**'\n",
        "  pull_request:\n",
        "    branches: [main]\n",
        "    paths:\n",
        "      - '02 Building & Integrating ML Pipelines/churn_classification_app_unittest/**'\n",
        "\n",
        "jobs:\n",
        "  test:\n",
        "    name: Run Unit, Integration, Functional, and E2E Tests\n",
        "    runs-on: ubuntu-latest\n",
        "\n",
        "    defaults:\n",
        "      run:\n",
        "        # Set the working directory to the project root within the repository\n",
        "        working-directory: \"02 Building & Integrating ML Pipelines/churn_classification_app_unittest\"\n",
        "\n",
        "    steps:\n",
        "    - name: Checkout Repository\n",
        "      uses: actions/checkout@v3\n",
        "\n",
        "    - name: Set up Python 3.11\n",
        "      uses: actions/setup-python@v4\n",
        "      with:\n",
        "        python-version: '3.11'\n",
        "\n",
        "    - name: Install Dependencies\n",
        "      run: |\n",
        "        pip install -r requirements.txt\n",
        "\n",
        "    - name: Run Unit Tests\n",
        "      run: |\n",
        "        # Run unit tests\n",
        "        # Assumes pure unit tests are in 'tests/unit/'\n",
        "        python -m unittest discover tests/unit -p \"test_*.py\"\n",
        "\n",
        "    - name: Run Integration Tests\n",
        "      run: |\n",
        "        # Run integration tests\n",
        "        # Assumes integration tests are in 'tests/integration/' and end with '_integration.py'\n",
        "        python -m unittest discover tests/integration -p \"test_*.py\"\n",
        "\n",
        "    - name: Run Functional Tests\n",
        "      run: |\n",
        "        # Run functional tests\n",
        "        # Assumes functional tests are in 'tests/functional/'\n",
        "        python -m unittest discover tests/functional -p \"test_*.py\"\n",
        "\n",
        "    - name: Run E2E Tests\n",
        "      run: |\n",
        "        # Run E2E tests\n",
        "        # Assumes E2E tests are in 'tests/e2e/'\n",
        "        python -m unittest discover tests/e2e -p \"test_*.py\"\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b485db47",
      "metadata": {},
      "source": [
        "Here an example of the requirements.txt for this project:\n",
        "\n",
        "```\n",
        "# Core ML Libraries\n",
        "pandas~=2.0.0      # For data manipulation and analysis\n",
        "numpy~=1.24.0      # For numerical operations\n",
        "scikit-learn~=1.2.0 # For machine learning models, preprocessing, and metrics\n",
        "joblib~=1.2.0      # For saving and loading Python objects, especially scikit-learn models\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2875f479",
      "metadata": {},
      "source": [
        "### Pre Push CI/CD Checklist\n",
        "\n",
        "Here's a checklist to run through before pushing your changes to the main branch, especially when that push is set to trigger your CI/CD pipeline on GitHub. This helps catch common issues before the automation kicks in.\n",
        "\n",
        "- [ ] Code Clean: All local changes committed, no temporary or debug code.\n",
        "\n",
        "- [ ] Tests Pass: All relevant local tests run successfully.\n",
        "\n",
        "- [ ] Dependencies Current: requirements.txt updated and tested if needed.\n",
        "\n",
        "- [ ] Docker Assets Valid: Dockerfile syntax is correct, and all files needed for the Docker build context (like app.py, src/, model_store/) are present.\n",
        "\n",
        "- [ ] Local Docker Test Passed: Docker image builds successfully locally, Docker container runs correctly locally, a quick local end to end test against the running container confirms basic app functionality (e.g., hitting /docs or a core endpoint).\n",
        "\n",
        "- [ ] Documentation Updated: README.md reflects any changes in usage or setup.\n",
        "\n",
        "- [ ] Workflow Valid: CI/CD workflow files (.yml) are syntactically correct if modified.\n",
        "\n",
        "- [ ] Clear Commit: Commit message concisely describes the changes.\n",
        "\n",
        "- [ ] Final Review: Briefly checked all changes and confirming push to main.\n",
        "\n",
        "\n",
        "After your checklist is complete:\n",
        "\n",
        "- Prepare Changes: \n",
        "\n",
        "    - `git add .` (stages all your modified files)\n",
        "\n",
        "    - `git commit -m \"Your concise commit message\"` (saves the changes locally with a note)\n",
        "\n",
        "- Send to GitHub:\n",
        "\n",
        "    - `git push origin main` (sends your committed changes to the main branch on GitHub)\n",
        "\n",
        "- On GitHub, immediately after your push:\n",
        "\n",
        "    - Your updated code appears in the repository.\n",
        "\n",
        "    - GitHub Actions detects the change.\n",
        "\n",
        "    - Your CI/CD workflow automatically starts running.\n",
        "\n",
        "    - You can watch its progress and results in the \"Actions\" tab of your GitHub repository.\n",
        "\n",
        "\n",
        "If no errors are detected... \n",
        "\n",
        "__Congratulations, you have deployed an ML CI/CD Process!__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027c626d",
      "metadata": {},
      "source": [
        "## README.md\n",
        "\n",
        "Serves as the front door to your project. It should be clear, comprehensive, and easy to navigate, allowing anyone (from a new collaborator to a future self) to quickly understand and engage with your work. `A good README.md` is like a clear project instruction manual and storefront. It should quickly tell you:\n",
        "\n",
        "- What it is: A brief project summary and its purpose.\n",
        "\n",
        "- How to set it up: Clear steps to get the code running locally, including dependencies (requirements.txt).\n",
        "\n",
        "- How to use it: Examples and commands to run the main functionalities.\n",
        "\n",
        "- What's inside (for ML): Basic info on the data and model, and key results.\n",
        "\n",
        "- Who made it & how to contribute: Information for collaborators and contact.\n",
        "\n",
        "- Legal bits: The project's license.\n",
        "\n",
        "- Essentially, it answers \"What is this?\", \"How do I get it working?\", and \"How do I use it?\".\n",
        "\n",
        "\n",
        "You can find an example of this file at the end of the notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d224f104",
      "metadata": {},
      "source": [
        "# Customer Churn Prediction API with Comprehensive Testing\n",
        "\n",
        "This project provides a production ready, containerized API for serving a customer churn prediction model. Built with FastAPI, it demonstrates MLOps best practices, including robust data validation with Pydantic and a comprehensive, layered testing strategy.\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "The API exposes a `/predict` endpoint that accepts customer data, feeds it to a pre trained scikit-learn pipeline, and returns a churn prediction along with the associated probabilities.\n",
        "\n",
        "The key highlight is the multi-layered test suite, which is structured to ensure code quality, reliability, and maintainability at every level of the application stack.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "The project is organized into a clear and modular structure:\n",
        "\n",
        "```\n",
        "churn_classification_app_unittest_training1/\n",
        " model_store/\n",
        "    churn_prediction_model_v1.joblib  # Pre-trained model artifact\n",
        " src/\n",
        "    config.py                         # Centralized configuration\n",
        " tests/\n",
        "    unit/\n",
        "       test_api_unit.py              # Tests Pydantic models in isolation\n",
        "    integration/\n",
        "       test_api_integration.py       # Tests API interaction with the real model\n",
        "    functional/\n",
        "       test_api_functional.py        # Tests API logic with a mocked model\n",
        "    e2e/\n",
        "        test_e2e.py                   # Tests the live, containerized application\n",
        " app.py                                # FastAPI application logic\n",
        " Dockerfile                            # Instructions to build the Docker image\n",
        " requirements.txt                      # Project dependencies\n",
        "```\n",
        "\n",
        "## Key Concepts Demonstrated\n",
        "\n",
        "*   **FastAPI**: For building the high performance, asynchronous API.\n",
        "*   **Pydantic**: For robust, type hint based data validation and serialization at the API boundary.\n",
        "*   **Docker**: For containerizing the application, its dependencies, and the ML model for consistent and reproducible deployments.\n",
        "*   **Layered Testing Strategy**: The project implements a full spectrum of tests, reflecting the \"Testing Pyramid\":\n",
        "    *   **Unit Tests**: Fast tests that verify the smallest piece of the API the Pydantic request model in complete isolation.\n",
        "    *   **Integration Tests**: Verifies the \"seam\" between the API layer and the actual, loaded machine learning model to ensure they are compatible.\n",
        "    *   **Functional Tests**: Checks the API's logic (routing, error handling, response formatting) by replacing the ML model with a mock, allowing for exhaustive testing of all API behaviors.\n",
        "    *   **End-to-End (E2E) Tests**: The highest level of testing. It builds the Docker image, runs the container, and sends real HTTP requests to the live service to validate the entire system.\n",
        "\n",
        "## Setup and Installation\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "1.  **Python 3.11+**\n",
        "2.  **Docker Desktop**: Required for running the end to end tests.\n",
        "3.  **Pre-trained Model**: This API requires a pre trained model file (`churn_prediction_model_v1.joblib`). Ensure this file is present in the `model_store/` directory. It can be generated by running the associated training pipeline project.\n",
        "\n",
        "### Installation Steps\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```bash\n",
        "    git clone <repository-url>\n",
        "    cd churn_classification_app_unittest_training1\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```bash\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install the dependencies:**\n",
        "    ```bash\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "## How to Run\n",
        "\n",
        "### Running the API Locally\n",
        "\n",
        "To run the FastAPI server locally for development, use `uvicorn`:\n",
        "\n",
        "```bash\n",
        "uvicorn app:app --reload\n",
        "```\n",
        "\n",
        "The API will be available at `http://127.0.0.1:8000`. You can access the interactive documentation (Swagger UI) at `http://127.0.0.1:8000/docs`.\n",
        "\n",
        "### Running the Tests\n",
        "\n",
        "The project uses Python's `unittest` framework. You can run all tests using the `discover` command from the project's root directory.\n",
        "\n",
        "**Run all tests (excluding E2E, which requires Docker):**\n",
        "```bash\n",
        "python -m unittest discover -s tests -p \"test_api_*.py\" -v\n",
        "```\n",
        "\n",
        "**Run a specific type of test:**\n",
        "\n",
        "*   **Unit Tests:**\n",
        "    ```bash\n",
        "    python -m unittest tests/unit/test_api_unit.py -v\n",
        "    ```\n",
        "*   **Integration Tests:**\n",
        "    ```bash\n",
        "    python -m unittest tests/integration/test_api_integration.py -v\n",
        "    ```\n",
        "*   **Functional Tests:**\n",
        "    ```bash\n",
        "    python -m unittest tests/functional/test_api_functional.py -v\n",
        "    ```\n",
        "\n",
        "*   **End-to-End Test (Requires Docker to be running):**\n",
        "    ```bash\n",
        "    python -m unittest tests/e2e/test_e2e.py -v\n",
        "    ```\n",
        "\n",
        "## API Endpoint\n",
        "\n",
        "### `POST /predict`\n",
        "\n",
        "This endpoint accepts customer data and returns a churn prediction.\n",
        "\n",
        "**Request Body:**\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tenure\": 1,\n",
        "  \"MonthlyCharges\": 29.85,\n",
        "  \"TotalCharges\": 29.85,\n",
        "  \"gender\": \"Female\",\n",
        "  \"SeniorCitizen\": 0,\n",
        "  \"Partner\": \"Yes\",\n",
        "  \"Dependents\": \"No\",\n",
        "  \"PhoneService\": \"No\",\n",
        "  \"MultipleLines\": \"No phone service\",\n",
        "  \"InternetService\": \"DSL\",\n",
        "  \"OnlineSecurity\": \"No\",\n",
        "  \"OnlineBackup\": \"Yes\",\n",
        "  \"DeviceProtection\": \"No\",\n",
        "  \"TechSupport\": \"No\",\n",
        "  \"StreamingTV\": \"No\",\n",
        "  \"StreamingMovies\": \"No\",\n",
        "  \"Contract\": \"Month-to-month\",\n",
        "  \"PaperlessBilling\": \"Yes\",\n",
        "  \"PaymentMethod\": \"Electronic check\"\n",
        "}\n",
        "```\n",
        "\n",
        "**Success Response (200 OK):**\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"prediction\": \"No Churn\",\n",
        "  \"churn_probability\": 0.0783,\n",
        "  \"no_churn_probability\": 0.9217,\n",
        "  \"probabilities\": {\n",
        "    \"No Churn\": 0.9217,\n",
        "    \"Churn\": 0.0783\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "This project serves as a robust template for building, testing, and deploying ML powered APIs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f50a00",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
